{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functiong說明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_and_open_excel 存成EXCEL 然後 打開\n",
    ">save_and_open_excel(dataframe, file_name=\"模型相關數值.xlsx\")\n",
    "\n",
    "analyze_nan_and_zero_values 檢視空值\n",
    ">analyze_nan_and_zero_values(data, threshold, sort_column=\"零值+NAN佔比 (%)\", ascending=False)\n",
    "\n",
    "output_coefficients 輸出模型係數\n",
    ">output_coefficients(model, poly_features, feature_names)\n",
    "\n",
    "plot_confusion_matrix 繪製混淆矩陣的熱力圖\n",
    ">plot_confusion_matrix(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_open_excel(dataframe, file_name=\"模型相關數值.xlsx\"):\n",
    "    \"\"\"\n",
    "    將給定的 dataframe 保存到指定的 Excel 工作表中，如果工作表已存在則覆蓋，\n",
    "    並嘗試在支持的應用程式中打開該 Excel 文件。\n",
    "\n",
    "    :param dataframe: 要保存的 pandas DataFrame。\n",
    "    :param file_name: Excel 文件的名稱，默認為 'vif_data.xlsx'。\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    # 獲取 dataframe 的變數名稱\n",
    "    frame_name = [name for name, var in globals().items() if var is dataframe][0]\n",
    "\n",
    "    # 檢查工作表是否存在，如果不存在則創建\n",
    "    if not os.path.isfile(file_name):\n",
    "        with pd.ExcelWriter(file_name, engine=\"openpyxl\") as writer:\n",
    "            dataframe.to_excel(writer, sheet_name=frame_name, index=False)\n",
    "    else:\n",
    "        # 工作表存在，加載工作簿\n",
    "        with pd.ExcelWriter(\n",
    "            file_name, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\"\n",
    "        ) as writer:\n",
    "            dataframe.to_excel(writer, sheet_name=frame_name, index=False)\n",
    "\n",
    "    # 嘗試打開創建的 Excel 文件\n",
    "    try:\n",
    "        os.startfile(file_name)\n",
    "    except AttributeError:\n",
    "        # 如果 os.startfile() 不可用（例如在非 Windows 系統上），使用適當的替代方案\n",
    "        if os.name == \"posix\":\n",
    "            os.system(f'open \"{file_name}\"')\n",
    "        else:\n",
    "            os.system(f'start \"{file_name}\"')\n",
    "    except Exception as e:\n",
    "        print(f\"打開 Excel 文件時發生錯誤: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"檢視空值及零值\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def analyze_nan_and_zero_values(\n",
    "    data, threshold, sort_column=\"零值+NAN佔比 (%)\", ascending=False\n",
    "):\n",
    "    \"\"\"\n",
    "    檢查0值和 nan 的數量\n",
    "    threshold 為 百分比，例threshold=20，會回傳 零值+NAN佔比 (%) 大於20% 的\n",
    "    當 threshold = 0 會回傳全部\n",
    "    \"\"\"\n",
    "\n",
    "    # 如果輸入是文件路徑，讀取CSV文件並轉換成Pandas DataFrame\n",
    "\n",
    "    if isinstance(data, str):\n",
    "\n",
    "        file_source_pd = pd.read_csv(data)\n",
    "\n",
    "    # 如果輸入是Pandas DataFrame，直接使用它\n",
    "\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "\n",
    "        file_source_pd = data\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise ValueError(\n",
    "            \"Invalid input type. Input must be either file path or Pandas DataFrame.\"\n",
    "        )\n",
    "\n",
    "    # 計算這個範圍內每一欄的空白值(NaN)數量\n",
    "\n",
    "    nan_values_per_column_in_range = file_source_pd.isnull().sum(axis=0)\n",
    "\n",
    "    # 計算每個欄位的總數據量\n",
    "\n",
    "    total_data_per_column = len(file_source_pd)\n",
    "\n",
    "    # 計算每個欄位的空白值(NaN)佔比\n",
    "\n",
    "    nan_percentage_per_column = (\n",
    "        nan_values_per_column_in_range / total_data_per_column\n",
    "    ) * 100\n",
    "\n",
    "    # 計算這個範圍內每一欄的零值數量\n",
    "\n",
    "    zero_values_per_column_in_range = (file_source_pd == 0).sum(axis=0)\n",
    "\n",
    "    # 計算每個欄位的零值佔比\n",
    "\n",
    "    zero_percentage_per_column = (\n",
    "        zero_values_per_column_in_range / total_data_per_column\n",
    "    ) * 100\n",
    "\n",
    "    # 計算每個欄位的零值和NaN值的總數\n",
    "\n",
    "    total_zero_and_nan_per_column = (\n",
    "        zero_values_per_column_in_range + nan_values_per_column_in_range\n",
    "    )\n",
    "\n",
    "    # 計算每個欄位的零值和NaN值總數的佔比\n",
    "\n",
    "    total_zero_and_nan_percentage_per_column = (\n",
    "        total_zero_and_nan_per_column / total_data_per_column\n",
    "    ) * 100\n",
    "\n",
    "    # 將結果轉換為 DataFrame\n",
    "\n",
    "    values_df = pd.DataFrame(\n",
    "        {\n",
    "            \"欄位名稱\": nan_values_per_column_in_range.index,\n",
    "            \"零值+NAN佔比 (%)\": total_zero_and_nan_percentage_per_column.values.round(\n",
    "                2\n",
    "            ),\n",
    "            \"空白值(NaN)數量\": nan_values_per_column_in_range.values,\n",
    "            \"空白值(NaN)佔比 (%)\": nan_percentage_per_column.values.round(2),\n",
    "            \"零值數量\": zero_values_per_column_in_range.values,\n",
    "            \"零值佔比 (%)\": zero_percentage_per_column.values.round(2),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 將 DataFrame 存儲到 CSV 檔案中\n",
    "    values_df.to_csv(\"零值與空白值統計.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # 如果 threshold 等於 0，則返回所有欄位\n",
    "    if threshold == 0:\n",
    "        return values_df\n",
    "\n",
    "    # 篩選出 '零值+NAN佔比 (%)' 大於指定閾值的欄位\n",
    "\n",
    "    filtered_values_df = values_df[values_df[\"零值+NAN佔比 (%)\"] > threshold]\n",
    "\n",
    "    # 根據指定的列進行排序\n",
    "\n",
    "    sorted_values_df = filtered_values_df.sort_values(\n",
    "        by=sort_column, ascending=ascending\n",
    "    )\n",
    "\n",
    "    return sorted_values_df\n",
    "\n",
    "\n",
    "# 測試\n",
    "\n",
    "\n",
    "# result_df = analyze_nan_and_zero_values(clean_data, threshold=0, sort_column='欄位名稱', ascending=False)\n",
    "\n",
    "\n",
    "# print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"輸出模型係數\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def output_coefficients(model, poly_features, feature_names):\n",
    "    \"\"\"\n",
    "    model：你的模型物件，通常是訓練好的機器學習模型，例如線性回歸模型、支持向量機模型等。\n",
    "\n",
    "    poly_features：多項式特徵物件，這通常是一個 PolynomialFeatures 物件，用於產生多項式特徵。\n",
    "\n",
    "    feature_names：特徵名稱列表，這是一個包含特徵名稱的字串列表，用於識別每個特徵\n",
    "    \"\"\"\n",
    "    coef = model.coef_\n",
    "    intercept = model.intercept_\n",
    "\n",
    "    # 創建包含係數和特徵名稱的DataFrame\n",
    "    coef_data = {\n",
    "        \"係數\": [coef[i] for i in range(len(coef)) if coef[i] != 0],\n",
    "        \"特徵\": [feature_names[i] for i in range(len(coef)) if coef[i] != 0],\n",
    "    }\n",
    "    coef_df = pd.DataFrame(coef_data)\n",
    "    coef_df = pd.concat(\n",
    "        [pd.DataFrame({\"係數\": [intercept], \"特徵\": [\"截距\"]}), coef_df],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    # 創建包含特徵名稱的DataFrame\n",
    "    feature_names_df = pd.DataFrame({\"特徵名稱\": feature_names})\n",
    "\n",
    "    # 輸出表格\n",
    "    print(\"係數和截距表：\")\n",
    "    print(coef_df)\n",
    "    print(\"\\n特徵名稱表：\")\n",
    "    print(feature_names_df)\n",
    "\n",
    "    # 將 DataFrame 存入 Excel\n",
    "    coef_df.to_excel(\"coefficients.xlsx\", index=False)\n",
    "\n",
    "    # 使用 os 模組打開 Excel 檔案\n",
    "    os.system(\"start coefficients.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"創建混合矩陣\"\"\"\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # 確保導入 numpy\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, figsize=(10, 7), cmap=\"Blues\"):\n",
    "    \"\"\"\n",
    "    繪製混淆矩陣的熱力圖。\n",
    "\n",
    "    參數:\n",
    "    y_true -- 真實標籤\n",
    "    y_pred -- 預測標籤\n",
    "    figsize -- 圖形的大小 (預設為 (10, 7))\n",
    "    cmap -- 熱力圖的顏色映射 (預設為 'Blues')\n",
    "    \"\"\"\n",
    "    # 生成混淆矩陣\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # 轉換為比例\n",
    "    cm_ratio = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # 使用 Seaborn 畫出混淆矩陣的熱力圖\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        cm_ratio, annot=True, fmt=\".2f\", cmap=cmap\n",
    "    )  # fmt='.2f' 指定顯示兩位小數\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel(\"Actual label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 使用此函數的示例\n",
    "# plot_confusion_matrix(y_test, y_pred_lr)  # 調用函數繪製混淆矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"二類分類指標\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 忽略警告用的\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# 解決字體問題\n",
    "\n",
    "plt.rcParams[\"font.family\"] = [\"Microsoft YaHei\"]\n",
    "\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model_multi_class(y_test, y_pred, figsize=(10, 7), cmap=\"Blues\"):\n",
    "\n",
    "    # 從y_test自動讀取類別數\n",
    "\n",
    "    num_classes = len(np.unique(y_test))\n",
    "\n",
    "\n",
    "    # 計算和顯示混淆矩陣\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    df_cm = pd.DataFrame(cm, index=range(num_classes), columns=range(num_classes))\n",
    "\n",
    "    sn.heatmap(df_cm, annot=True, fmt=\"g\")\n",
    "\n",
    "    plt.title(\"Confusion matrix (混淆矩陣)\\n\", y=1.1)\n",
    "\n",
    "    plt.ylabel(\"Actual label (實際標籤)\\n\")\n",
    "\n",
    "    plt.xlabel(\"Predicted label (預測標籤)\\n\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # 計算和顯示每個類的Accuracy、Precision、Recall和F1 Score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    print(f\"Accuracy (準確率): {accuracy}\")\n",
    "\n",
    "    print(f\"Precision (精確率) - Macro Average: {precision}\")\n",
    "\n",
    "    print(f\"Recall (召回率) - Macro Average: {recall}\")\n",
    "\n",
    "    print(f\"F1 Score (F1分數) - Macro Average: {f1}\")\n",
    "\n",
    "\n",
    "\n",
    "# 使用範例\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate_model_multi_class(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 輸入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# file_path = r\"..\\飲料店總表0307final01_補上人氣_補值_xlsxclustered_HG_data.xlsx\"\n",
    "file_path = r\"..\\飲料店總表0307final01_補上人氣_補值_hg.xlsx\"\n",
    "# dataset = pd.read_csv(file_path, sep=\",\", encoding=\"UTF-8\")\n",
    "\n",
    "dataset = pd.read_excel(file_path)\n",
    "# print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_nan_and_zero_values(dataset, 0, sort_column=\"零值+NAN佔比 (%)\", ascending=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 選擇需要的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"selected_data\n",
    "\"\"\"\n",
    "\n",
    "selected_data = dataset[\n",
    "    [\n",
    "        \"star\",\n",
    "        \"school_counts\",\n",
    "        \"drink_counts\",\n",
    "        \"train_counts\",\n",
    "        \"youbike_counts\",\n",
    "        \"bus_counts\",\n",
    "        \"park_counts\",\n",
    "        \"night_market_counts\",\n",
    "        \"sports_facilities_counts\",\n",
    "        \"mrt_counts\",\n",
    "        \"movie_theater_counts\",\n",
    "        \"hospital_counts\",\n",
    "        \"salary_income_median\",\n",
    "        \"people_flow_mean\",\n",
    "        \"knock_down_price_mean\",\n",
    "        \"weekend_open\",\n",
    "        \"road_area_ratio\",\n",
    "        \"age\",\n",
    "        \"weekday_working_hours_average\",\n",
    "        # \"comment\",\n",
    "        # \"people_flow_average\",\n",
    "        \"popularity\",\n",
    "        # \"KMEANS\",\n",
    "    ]\n",
    "]\n",
    "# selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算相關矩陣\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corrmat = dataset.corr()\n",
    "\n",
    "# 繪製熱力圖\n",
    "plt.figure(figsize=(28, 24))\n",
    "sns.heatmap(corrmat, annot=True, annot_kws={\"size\": 12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顯示關係性太低的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"篩選關係姓小的\"\"\"\n",
    "\n",
    "# 計算與 'KMEANS' 欄位相關性的系列\n",
    "corr_with_kmeans = selected_data.corr()[\"popularity\"]\n",
    "\n",
    "# 篩選出與 'KMEANS' 相關性小於0.1的欄位\n",
    "low_corr_with_kmeans = corr_with_kmeans[abs(corr_with_kmeans) < 0.1]\n",
    "\n",
    "# 列出這些欄位的名稱\n",
    "print(\"與 'popularity' 欄位相關性小於0.1的欄位：\")\n",
    "print(low_corr_with_kmeans.index.tolist())\n",
    "selected_data = selected_data.drop(low_corr_with_kmeans.index.tolist(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"將連續數值popularity 轉成 類別數值 popularity_category\n",
    "\"\"\"\n",
    "\n",
    "# 1. 轉換 object 資料類型的欄位\n",
    "dataset[\"name\"] = dataset[\"name\"].astype(\"category\")\n",
    "dataset[\"class\"] = dataset[\"class\"].astype(\"category\")\n",
    "dataset[\"address\"] = dataset[\"address\"].astype(\"category\")\n",
    "dataset[\"district\"] = dataset[\"district\"].astype(\"category\")\n",
    "dataset[\"neighborhood\"] = dataset[\"neighborhood\"].astype(\"category\")\n",
    "dataset[\"brand\"] = dataset[\"brand\"].astype(\"category\")\n",
    "\n",
    "\n",
    "# '''將缺失值補成-1'''\n",
    "# dataset['brand'] = dataset['brand'].fillna(-1)\n",
    "# dataset['Saturday_open_hours'] = dataset['Saturday_open_hours'].fillna(-1)\n",
    "# dataset['Sunday_open_hours'] = dataset['Sunday_open_hours'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"依照數量\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 假設的 selected_data 和 'popularity' 欄位數據\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# 使用 pd.qcut 進行分類\n",
    "Y_classification_pd = pd.DataFrame({\"value\": selected_data[\"popularity\"]})\n",
    "\n",
    "\n",
    "Y_classification_pd[\"category\"] = pd.qcut(\n",
    "    Y_classification_pd[\"value\"],\n",
    "    q=5,\n",
    "    labels=[\"Category 0\", \"Category 1\", \"Category 2\", \"Category 3\", \"Category 4\"],\n",
    ")\n",
    "\n",
    "\n",
    "# 分組並獲得統計信息\n",
    "summary_df = (\n",
    "    Y_classification_pd.groupby(\"category\")[\"value\"]\n",
    "    .agg([(\"最小值\", \"min\"), (\"最大值\", \"max\"), (\"數量\", \"size\")])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "# 建立分類\n",
    "value_category_ranges = [\n",
    "    summary_df.loc[0, \"最小值\"],  # 第一類別的最小值\n",
    "    summary_df.loc[1, \"最小值\"],  # 第二類別的最小值\n",
    "    summary_df.loc[2, \"最小值\"],  # 第三類別的最小值\n",
    "    summary_df.loc[3, \"最小值\"],  # 第四類別的最小值\n",
    "    summary_df.loc[4, \"最小值\"],  # 第五類別的最小值\n",
    "    summary_df.loc[4, \"最大值\"],  # 第五類別的最大值\n",
    "]\n",
    "\n",
    "\n",
    "# 使用 pd.cut 根據新的範圍劃分 'popularity' 欄位\n",
    "selected_data[\"popularity_category\"] = pd.cut(\n",
    "    selected_data[\"popularity\"],\n",
    "    bins=value_category_ranges,\n",
    "    right=False,  # 包括左邊界，排除右邊界\n",
    "    labels=[0, 1, 2, 3, 4],\n",
    ")\n",
    "\n",
    "\n",
    "# 將超出範圍的值填充為最大範圍的類別（4）\n",
    "selected_data[\"popularity_category\"].fillna(4, inplace=True)\n",
    "\n",
    "\n",
    "# 將 'popularity_category' 列轉換為整數類型\n",
    "selected_data[\"popularity_category\"] = selected_data[\"popularity_category\"].astype(int)\n",
    "\n",
    "\n",
    "# 打印新的分類結果和每類的數量\n",
    "print(selected_data[\"popularity_category\"].value_counts().sort_index())\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"因為數值分布差很多 刪掉極端\"\"\"\n",
    "\n",
    "# 直接在原始 DataFrame 中刪除 'popularity' 小於 8726.779 的行\n",
    "selected_data = selected_data[selected_data[\"popularity\"] <= 2213.641425]\n",
    "# selected_data = selected_data[selected_data[\"popularity\"] != 0]\n",
    "\n",
    "# 顯示過濾後的數據\n",
    "# print(selected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 使用 pd.cut 根據數值大小劃分 'popularity' 欄位\n",
    "\n",
    "# 計算數據範圍\n",
    "\n",
    "min_val = selected_data[\"popularity\"].min()\n",
    "\n",
    "max_val = selected_data[\"popularity\"].max()\n",
    "\n",
    "\n",
    "# 創建五個分組的邊界值\n",
    "\n",
    "bins = np.linspace(min_val, max_val, 6)\n",
    "\n",
    "\n",
    "# 使用 pd.cut 來分組\n",
    "\n",
    "selected_data[\"popularity_category\"] = pd.cut(\n",
    "    selected_data[\"popularity\"],\n",
    "    bins=bins,\n",
    "    include_lowest=True,  # 確保包括最低值\n",
    "    labels=[0, 1, 2, 3, 4],  # 這是每個範圍的標籤\n",
    ")\n",
    "\n",
    "\n",
    "# 打印新的分類結果和每類的數量\n",
    "\n",
    "# print(selected_data['popularity_category'].value_counts().sort_index())\n",
    "\n",
    "\n",
    "# 建立統計信息表格\n",
    "\n",
    "summary_df = (\n",
    "    selected_data.groupby(\"popularity_category\")[\"popularity\"]\n",
    "    .agg(最小值=\"min\", 最大值=\"max\", 數量=\"size\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"轉位數\"\"\"\n",
    "\n",
    "selected_data[\"age\"] = selected_data[\"age\"].round(2)\n",
    "selected_data[\"road_area_ratio\"] = selected_data[\"road_area_ratio\"].round(3)\n",
    "\n",
    "\n",
    "# dataset['brand'] = dataset['brand'].fillna(-1)\n",
    "# dataset['Saturday_open_hours'] = dataset['Saturday_open_hours'].fillna(-1)\n",
    "# dataset['Sunday_open_hours'] = dataset['Sunday_open_hours'].fillna(-1)\n",
    "\n",
    "\n",
    "# X = dataset.drop(\n",
    "#     ['comment','star','people_flow_average','popularity',\"KMEANS\"], axis=1\n",
    "# )\n",
    "y = selected_data[\"popularity_category\"]\n",
    "\n",
    "X = selected_data.drop([\"popularity\", \"popularity_category\"], axis=1)\n",
    "\n",
    "# y = dataset[\"KMEANS\"]\n",
    "y = y.to_frame()\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 輸入前最後一次確認參數型態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_nan_and_zero_values(X, 0, sort_column=\"零值+NAN佔比 (%)\", ascending=False)\n",
    "# analyze_nan_and_zero_values(y, 0, sort_column=\"零值+NAN佔比 (%)\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割訓練和測試\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# test_size 切的尺寸 30% random_state讓抽取可以是穩定的結果(第一次抽根第十次抽是一樣的)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 忽略警告用的\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# 導入並訓練邏輯回歸(Logistic Regression)模型\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集並計算準確率\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "classification_report_lr = classification_report(y_test, y_pred_lr)\n",
    "\n",
    "# 輸出結果\n",
    "print(accuracy_lr)\n",
    "print(classification_report_lr)\n",
    "\n",
    "# plot_confusion_matrix(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混淆矩陣(Confusion Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 使用模型預測測試集\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# 生成混淆矩陣\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "\n",
    "# 轉換為比例\n",
    "cm_ratio = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# 使用Seaborn畫出混淆矩陣的熱力圖\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_ratio, annot=True, fmt=\".2f\", cmap=\"Blues\")  # fmt='.2f' 指定顯示兩位小數\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"Actual label\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉驗證(Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 忽略警告用的\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 定義模型\n",
    "lr_model_cv = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# 進行5-fold交叉驗證\n",
    "scores = cross_val_score(lr_model_cv, X_train, y_train, cv=5)\n",
    "\n",
    "# 輸出每一輪的準確率以及平均準確率\n",
    "print(\"每一輪的準確率:\", scores)\n",
    "print(\"平均準確率:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "C_values = np.logspace(-4, 3, 10)\n",
    "\n",
    "# 定義原始參數範圍和模型（僅使用 L1 和 L2 正則化）\n",
    "original_param_grid = {\"C\": C_values, \"penalty\": [\"l1\", \"l2\"]}\n",
    "original_grid_search = GridSearchCV(\n",
    "    LogisticRegression(\n",
    "        max_iter=1000, random_state=42, solver=\"liblinear\"\n",
    "    ),  # 'liblinear' 支持 'l1' 和 'l2'\n",
    "    original_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "\n",
    "# 執行原始網格搜索\n",
    "original_grid_search.fit(X_train, y_train)\n",
    "original_best_params = original_grid_search.best_params_\n",
    "original_best_score = original_grid_search.best_score_\n",
    "\n",
    "# 定義 Elastic Net 參數範圍和模型\n",
    "elastic_param_grid = {\n",
    "    \"C\": C_values,\n",
    "    \"l1_ratio\": np.linspace(0, 1, 5),  # 從 0 到 1 均勻分布的 5 個點\n",
    "}\n",
    "elastic_grid_search = GridSearchCV(\n",
    "    LogisticRegression(\n",
    "        max_iter=1000, random_state=42, penalty=\"elasticnet\", solver=\"saga\"\n",
    "    ),\n",
    "    elastic_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "\n",
    "# 執行 Elastic Net 網格搜索\n",
    "elastic_grid_search.fit(X_train, y_train)\n",
    "elastic_best_params = elastic_grid_search.best_params_\n",
    "elastic_best_score = elastic_grid_search.best_score_\n",
    "\n",
    "# 比較結果並顯示\n",
    "print(\"原始最佳參數:\", original_best_params)\n",
    "print(\"原始最高準確率:\", original_best_score)\n",
    "print(\"Elastic Net 最佳參數:\", elastic_best_params)\n",
    "print(\"Elastic Net 最高準確率:\", elastic_best_score)\n",
    "\n",
    "# 判斷哪一個模型更好\n",
    "if original_best_score > elastic_best_score:\n",
    "    print(\"原始模型較好。\")\n",
    "else:\n",
    "    print(\"Elastic Net 模型較好。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 解決字體問題\n",
    "plt.rcParams[\"font.family\"] = [\"Microsoft YaHei\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 獲取參數組合和對應的準確率\n",
    "params = original_grid_search.cv_results_[\"params\"]\n",
    "mean_scores = original_grid_search.cv_results_[\"mean_test_score\"]\n",
    "\n",
    "# 繪製原始模型的準確率與 C 值的關係圖表\n",
    "plt.figure(figsize=(10, 6))\n",
    "for penalty, marker in zip([\"l1\", \"l2\"], [\"o\", \"s\"]):\n",
    "    penalty_mask = [param[\"penalty\"] == penalty for param in params]\n",
    "    plt.plot(\n",
    "        C_values, mean_scores[penalty_mask], marker=marker, label=f\"penalty={penalty}\"\n",
    "    )\n",
    "\n",
    "    # 標記每個參數組合\n",
    "    for i, c in enumerate(C_values):\n",
    "        plt.text(\n",
    "            c,\n",
    "            mean_scores[penalty_mask][i],\n",
    "            f\"C={c:.2f}\",\n",
    "            fontsize=8,\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"C 值\")\n",
    "plt.ylabel(\"平均準確率\")\n",
    "plt.title(\"C 值與平均準確率的關係\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用最好的C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"使用最好的C\"\"\"\n",
    "\n",
    "# 導入並訓練邏輯回歸(Logistic Regression)模型\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 使用原始模型的最佳參數來設定模型\n",
    "lr_model_best = LogisticRegression(\n",
    "    C=278.2559402207126,\n",
    "    penalty=\"l1\",\n",
    "    solver=\"liblinear\",\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    ")\n",
    "lr_model_best.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集並計算準確率\n",
    "y_pred_best_lr = lr_model_best.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_best_lr)\n",
    "classification_report_lr = classification_report(y_test, y_pred_best_lr)\n",
    "\n",
    "# 輸出結果\n",
    "print(\"準確率:\", accuracy_lr)\n",
    "print(\"分類報告:\\n\", classification_report_lr)\n",
    "\n",
    "\"\"\"建立混淆矩陣\"\"\"\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_best_lr)\n",
    "\n",
    "\n",
    "\"\"\"交叉驗證(Cross-Validation)\"\"\"\n",
    "# 進行5-fold交叉驗證\n",
    "scores = cross_val_score(lr_model_best, X_train, y_train, cv=5)\n",
    "\n",
    "# 輸出每一輪的準確率以及平均準確率\n",
    "print(\"每一輪的準確率:\", scores)\n",
    "print(\"平均準確率:\", scores.mean())\n",
    "\n",
    "\"\"\"輸出模型\"\"\"\n",
    "from joblib import dump\n",
    "\n",
    "# 保存模型\n",
    "dump(lr_model_best, \"lr_model_best.joblib\")\n",
    "\n",
    "\n",
    "evaluate_model_multi_class(y_test, y_pred_best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型係數(Coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看模型係數\n",
    "import pandas as pd\n",
    "\n",
    "feature_names = X_train.columns\n",
    "coefficients = lr_model_best.coef_[0]\n",
    "feature_importance = pd.DataFrame(\n",
    "    {\"Feature\": feature_names, \"Coefficient\": coefficients}\n",
    ")\n",
    "feature_importance = feature_importance.sort_values(by=\"Coefficient\", ascending=False)\n",
    "print(feature_importance)\n",
    "\n",
    "# 確保DataFrame已經根據Coefficient進行降序排序\n",
    "feature_importance_sorted = feature_importance.sort_values(\n",
    "    by=\"Coefficient\", ascending=False\n",
    ")\n",
    "\n",
    "# 繪製特徵重要性的水平條形圖，重要性較高的特徵會顯示在最上面\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(\n",
    "    feature_importance_sorted[\"Feature\"][:10],\n",
    "    feature_importance_sorted[\"Coefficient\"][:10],\n",
    ")\n",
    "plt.xlabel(\"Coefficient\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Top 10 Features Importance - Logistic Regression\")\n",
    "plt.gca().invert_yaxis()  # 確保重要性較高的特徵顯示在最上面\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# 訓練支持向量機模型，默認 高斯核（RBF核）\n",
    "# svm_model = SVC(kernel='rbf', random_state=42)\n",
    "svm_model = SVC(random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# 計算準確率和顯示分類報告\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "classification_report_svm = classification_report(y_test, y_pred_svm)\n",
    "\n",
    "print(accuracy_svm)\n",
    "print(classification_report_svm)\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉驗證(Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 初始化支持向量機模型\n",
    "svm_model = SVC(random_state=42)\n",
    "\n",
    "# 進行交叉驗證，這裡假設使用5折交叉驗證\n",
    "scores = cross_val_score(svm_model, X_train, y_train, cv=5)\n",
    "\n",
    "# 打印出每一折的準確率以及平均準確率\n",
    "print(\"每一折的準確率: \", scores)\n",
    "print(\"交叉驗證結果：平均準確率\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 參數調整(Parameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "grid_C_values = [0.1, 1, 10, 100]\n",
    "dist_C_values = reciprocal(0.1, 100)\n",
    "\n",
    "# 設定參數範圍和分佈\n",
    "param_grid = {\"C\": grid_C_values, \"gamma\": [1, 0.1, 0.01, 0.001]}\n",
    "param_dist = {\"C\": dist_C_values, \"gamma\": expon(scale=1.0)}\n",
    "\n",
    "# 創建 GridSearchCV 和 RandomizedSearchCV 物件\n",
    "grid_search = GridSearchCV(SVC(), param_grid, refit=True, verbose=0)\n",
    "random_search = RandomizedSearchCV(\n",
    "    SVC(), param_distributions=param_dist, n_iter=100, refit=True, verbose=0\n",
    ")\n",
    "\n",
    "# 執行擬合\n",
    "grid_search.fit(X_train, y_train)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 整理出圖的數據\n",
    "results_grid = grid_search.cv_results_\n",
    "results_random = random_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示最佳參數和最佳準確率\n",
    "best_grid_params = grid_search.best_params_\n",
    "best_random_params = random_search.best_params_\n",
    "best_grid_score = grid_search.best_score_\n",
    "best_random_score = random_search.best_score_\n",
    "\n",
    "\n",
    "# 比較準確率並指出哪個更高，然後創建相應的 SVC 實例\n",
    "if best_grid_score > best_random_score:\n",
    "    print(\"最佳參數 (Grid Search CV): \", best_grid_params)\n",
    "    print(\"最佳準確率 (Grid Search CV): \", best_grid_score)\n",
    "    # 直接使用具體的參數值來創建 SVC 實例\n",
    "    svc_best = SVC(C=best_grid_params[\"C\"], gamma=best_grid_params[\"gamma\"])\n",
    "elif best_grid_score < best_random_score:\n",
    "    print(\"最佳參數 (Randomized Search CV): \", best_random_params)\n",
    "    print(\"最佳準確率 (Randomized Search CV): \", best_random_score)\n",
    "    # 直接使用具體的參數值來創建 SVC 實例\n",
    "    svc_best = SVC(C=best_random_params[\"C\"], gamma=best_random_params[\"gamma\"])\n",
    "else:\n",
    "    print(\"兩種搜索方法提供了相同的準確率。\")\n",
    "    # 可以選擇任何一組參數來創建 SVC 實例，這裡選擇了 Grid Search 的參數\n",
    "    svc_best = SVC(C=best_grid_params[\"C\"], gamma=best_grid_params[\"gamma\"])\n",
    "\n",
    "\"\"\"交叉驗證\"\"\"\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 進行交叉驗證\n",
    "cross_val_scores_best = cross_val_score(svc_best, X_train, y_train, cv=5)\n",
    "\n",
    "\n",
    "# 使用最佳參數和全部訓練數據來訓練 SVC\n",
    "svc_best.fit(X_train, y_train)\n",
    "\n",
    "# 現在 svc_best 是被訓練過的，可以計算交叉驗證分數\n",
    "cross_val_scores_best = cross_val_score(svc_best, X_train, y_train, cv=5)\n",
    "\n",
    "# 計算平均交叉驗證準確率\n",
    "avg_score_best = np.mean(cross_val_scores_best)\n",
    "print(\"平均交叉驗證準確率: \", avg_score_best)\n",
    "\n",
    "\n",
    "\"\"\"輸出模型\"\"\"\n",
    "from joblib import dump\n",
    "\n",
    "# 保存模型\n",
    "dump(svc_best, \"svc_best.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_best_SVC = lr_model_best.predict(X_test)\n",
    "\n",
    "evaluate_model_multi_class(y_test, y_pred_best_SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 訓練 AdaBoost 模型\n",
    "# 這裡我們設置 50 個弱學習器，並且使用決策樹作為默認的基學習器\n",
    "ada_model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "ada_model.fit(X_train, y_train)\n",
    "\n",
    "# 進行預測\n",
    "y_pred_ada_transformed = ada_model.predict(X_test)\n",
    "\n",
    "# 計算準確率和其他性能指標\n",
    "accuracy_ada = accuracy_score(y_test, y_pred_ada_transformed)\n",
    "classification_report_ada = classification_report(y_test, y_pred_ada_transformed)\n",
    "\n",
    "# 將預測結果轉換回原始範圍（1~5）\n",
    "y_pred_ada_original = y_pred_ada_transformed + 1\n",
    "\n",
    "# 輸出結果\n",
    "print(f\"Accuracy (準確率): {accuracy_ada}\")\n",
    "print(\"Classification Report (分類報告):\\n\", classification_report_ada)\n",
    "\n",
    "# 如果您需要使用預測結果進行其他操作，請使用 y_pred_ada_original\n",
    "\n",
    "# 計算混淆矩陣\n",
    "cm = confusion_matrix(y_test, y_pred_ada_transformed)\n",
    "\n",
    "# 轉換為比例\n",
    "cm_ratio = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# # 使用Seaborn畫出混淆矩陣的熱力圖\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# sns.heatmap(cm_ratio, annot=True, fmt=\".2f\", cmap=\"Blues\")  # fmt='.2f' 指定顯示兩位小數\n",
    "# plt.title(\"Confusion Matrix (混淆矩陣)\")\n",
    "# plt.ylabel(\"Actual label (實際標籤)\")\n",
    "# plt.xlabel(\"Predicted label (預測標籤)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉驗證(Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 定義 AdaBoost 分類器\n",
    "ada_model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# 進行交叉驗證，cv 參數指定交叉驗證的折數\n",
    "# 如果資料集較小，您可以考慮使用較大的折數，例如 cv=10\n",
    "# 請確保 X 和 y 是您的特徵和標籤資料\n",
    "# 這個過程可能會花費一些時間，特別是當資料集很大時\n",
    "cv_scores = cross_val_score(ada_model, X, y, cv=5)\n",
    "\n",
    "# 輸出交叉驗證結果\n",
    "print(\"交叉驗證準確率:\", cv_scores)\n",
    "print(\"平均交叉驗證準確率:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 參數調整(Parameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 忽略警告用的\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 定義參數範圍\n",
    "param_grid_ada = {\n",
    "    \"n_estimators\": [10, 50, 100, 200],\n",
    "    \"learning_rate\": [0.01, 0.1, 1, 10],\n",
    "}\n",
    "param_dist_ada = {\"n_estimators\": randint(50, 500), \"learning_rate\": [0.01, 0.1, 1, 10]}\n",
    "\n",
    "# 創建 GridSearchCV 和 RandomizedSearchCV 物件\n",
    "grid_search_ada = GridSearchCV(\n",
    "    AdaBoostClassifier(random_state=42), param_grid_ada, refit=True, verbose=0\n",
    ")\n",
    "random_search_ada = RandomizedSearchCV(\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    param_distributions=param_dist_ada,\n",
    "    n_iter=100,\n",
    "    refit=True,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# 執行擬合\n",
    "grid_search_ada.fit(X_train, y_train)\n",
    "random_search_ada.fit(X_train, y_train)\n",
    "\n",
    "# 選擇最佳模型\n",
    "best_model_grid_ada = grid_search_ada.best_estimator_\n",
    "best_model_random_ada = random_search_ada.best_estimator_\n",
    "\n",
    "# 打印最佳參數\n",
    "print(\n",
    "    \"Best parameters (GridSearchCV) (最佳參數-網格搜索):\", grid_search_ada.best_params_\n",
    ")\n",
    "print(\n",
    "    \"Best parameters (RandomizedSearchCV) (最佳參數-隨機搜索):\",\n",
    "    random_search_ada.best_params_,\n",
    ")\n",
    "\n",
    "# 使用最佳模型進行預測\n",
    "y_pred_ada_best_grid = best_model_grid_ada.predict(X_test)\n",
    "y_pred_ada_best_random = best_model_random_ada.predict(X_test)\n",
    "\n",
    "# 您可以基於 y_pred_ada_best_grid 和 y_pred_ada_best_random 進行進一步的分析和評估\n",
    "\n",
    "# 計算準確率\n",
    "accuracy_ada_best_grid = accuracy_score(y_test, y_pred_ada_best_grid)\n",
    "accuracy_ada_best_random = accuracy_score(y_test, y_pred_ada_best_random)\n",
    "\n",
    "# 打印準確率\n",
    "print(\"Accuracy (GridSearchCV) (準確率-網格搜索):\", accuracy_ada_best_grid)\n",
    "print(\"Accuracy (RandomizedSearchCV) (準確率-隨機搜索):\", accuracy_ada_best_random)\n",
    "\n",
    "# 比較準確率並選擇最好的模型\n",
    "if accuracy_ada_best_grid > accuracy_ada_best_random:\n",
    "    AdaBoost_model_best = best_model_grid_ada\n",
    "    print(\"GridSearchCV 的模型表現較好。\")\n",
    "else:\n",
    "    AdaBoost_model_best = best_model_random_ada\n",
    "    print(\"RandomizedSearchCV 的模型表現較好。\")\n",
    "\n",
    "# 使用最佳模型進行其他操作\n",
    "# 例如，您可以使用 best_model 進行更多預測或進行深入分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 忽略警告用的\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 使用交叉驗證評估模型\n",
    "cross_val_scores = cross_val_score(AdaBoost_model_best, X, y, cv=5)  # 使用5折交叉驗證\n",
    "\n",
    "# 打印每次交叉驗證的準確度\n",
    "print(\"Cross Validation Scores:\", cross_val_scores)\n",
    "# 打印平均交叉驗證準確度\n",
    "print(\"平均準確率:\", cross_val_scores.mean())\n",
    "\n",
    "# 使用最佳模型進行預測\n",
    "y_pred_best_ada = AdaBoost_model_best.predict(X_test)\n",
    "\n",
    "\n",
    "evaluate_model_multi_class(y_test, y_pred_best_ada)\n",
    "plot_confusion_matrix(y_test, y_pred_best_ada)\n",
    "\"\"\"輸出模型\"\"\"\n",
    "from joblib import dump, load\n",
    "\n",
    "# 保存模型\n",
    "dump(AdaBoost_model_best, \"AdaBoost_model_best.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设 AdaBoost_model_best 是您的训练好的模型\n",
    "feature_names = X_train.columns  # X_train 是训练数据集\n",
    "importances = AdaBoost_model_best.feature_importances_  # 获取特征重要性\n",
    "feature_importance = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n",
    "\n",
    "# 按照重要性进行降序排列\n",
    "feature_importance = feature_importance.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# 显示排序后的DataFrame\n",
    "print(feature_importance)\n",
    "\n",
    "# 绘制特征重要性的水平条形图，重要性较高的特征会显示在最上面\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(\n",
    "    feature_importance[\"Feature\"][:10],  # 只显示前10个特征\n",
    "    feature_importance[\"Importance\"][:10],\n",
    ")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Top 10 Feature Importance - AdaBoost\")\n",
    "plt.gca().invert_yaxis()  # 确保重要性较高的特征显示在最上面\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "# 建立XGBoost模型\n",
    "model = xgb.XGBClassifier(\n",
    "    enable_categorical=True,\n",
    ")\n",
    "# 訓練模型\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    ")\n",
    "\n",
    "# 計算準確率(Accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"準確率: {accuracy}\")  # 印出準確率\n",
    "\n",
    "# 計算精確率(Precision)\n",
    "precision = precision_score(\n",
    "    y_test, y_pred, average=\"weighted\"\n",
    ")  # 使用 'weighted' 方法來處理類別不平衡的情況\n",
    "print(f\"精確率: {precision}\")  # 印出精確率\n",
    "\n",
    "# 計算召回率(Recall)\n",
    "recall = recall_score(\n",
    "    y_test, y_pred, average=\"weighted\"\n",
    ")  # 使用 'weighted' 方法來處理類別不平衡的情況\n",
    "print(f\"召回率: {recall}\")  # 印出召回率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉驗證(Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# 定義 XGBoost 模型\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softmax\", num_class=5, random_state=42)\n",
    "\n",
    "\n",
    "# 執行交叉驗證\n",
    "\n",
    "# cv 參數決定折數，例如，cv=5 代表 5 折交叉驗證\n",
    "\n",
    "# scoring 參數可以根據需要更改，例如使用 'accuracy' 來獲取準確率\n",
    "\n",
    "\n",
    "y_xgb = le.fit_transform(y)  # 轉換目標變數\n",
    "\n",
    "scores = cross_val_score(xgb_model, X, y_xgb, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "\n",
    "# 輸出結果\n",
    "\n",
    "print(\"每一輪的準確率:\", scores)\n",
    "\n",
    "print(\"平均準確率:\", scores.mean())\n",
    "\n",
    "# print('Standard deviation of accuracy:', scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 參數調整(Parameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# 定義超參數網格\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 4, 5],  # 最大深度\n",
    "    \"learning_rate\": [0.1, 0.01, 0.001],  # 學習率\n",
    "    \"n_estimators\": [100, 200, 300],  # 樹的數量\n",
    "    \"objective\": [\"multi:softmax\", \"multi:softprob\"],  # 目標函數\n",
    "    \"subsample\": [0.6, 0.8, 1],  # 子樣本比例\n",
    "    \"colsample_bytree\": [0.8, 1, 1.2],  # 每棵樹隨機選擇特徵的比例\n",
    "}\n",
    "\n",
    "# 初始化XGBoost分類器\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, num_class=5)\n",
    "\n",
    "# 設置GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_classifier,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "# 擬合GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 輸出最佳參數\n",
    "print(\"找到的最佳參數: \", grid_search.best_params_)\n",
    "\n",
    "# 輸出最佳準確率\n",
    "print(\"找到的最佳準確率: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用最好的參數做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用最佳參數對整個訓練集進行重新訓練\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# 進行預測\n",
    "y_pred_best_XG = best_xgb_model.predict(X_test)\n",
    "\n",
    "# 計算準確率和其他性能指標\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best_XG)\n",
    "classification_report_best = classification_report(y_test, y_pred_best_XG)\n",
    "\n",
    "# 輸出結果\n",
    "print(f\"最佳模型準確率: {accuracy_best}\")\n",
    "print(\"最佳模型分類報告:\\n\", classification_report_best)\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_best_XG)\n",
    "\n",
    "\"\"\"輸出模型\"\"\"\n",
    "from joblib import dump, load\n",
    "\n",
    "# 保存模型\n",
    "dump(best_xgb_model, \"XGBoost model_best.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徵重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_names = X_train.columns\n",
    "importances = best_xgb_model.feature_importances_\n",
    "feature_importance = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n",
    "\n",
    "# 按照重要性進行降序排列\n",
    "feature_importance = feature_importance.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# 顯示排序後的DataFrame\n",
    "print(feature_importance)\n",
    "\n",
    "\n",
    "# 繪製特徵重要性的水平條形圖，重要性較高的特徵會顯示在最上面\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(\n",
    "    feature_importance[\"Feature\"][:10],\n",
    "    feature_importance[\"Importance\"][:10],\n",
    ")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Top 10 Feature Importance - XGBoost\")\n",
    "plt.gca().invert_yaxis()  # 確保重要性較高的特徵顯示在最上面\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文化圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立英文特徵名稱到中文的映射字典\n",
    "feature_name_map = {\n",
    "    \"knock_down_price_mean\": \"地區租屋平均值\",\n",
    "    \"road_area_ratio\": \"道路面積比率\",\n",
    "    \"mrt_counts\": \"捷運站數量\",\n",
    "    \"weekend_open\": \"週末開放\",\n",
    "    \"star\": \"星級\",\n",
    "    \"sports_facilities_counts\": \"體育設施數量\",\n",
    "    \"people_flow_mean\": \"人流量平均值\",\n",
    "    \"salary_income_median\": \"薪水收入中位數\",\n",
    "    \"youbike_counts\": \"YouBike站點數量\",\n",
    "    \"weekday_working_hours_average\": \"工作日平均工時\",\n",
    "}\n",
    "\n",
    "# 替換DataFrame中的特徵名稱\n",
    "feature_importance[\"Feature\"] = feature_importance[\"Feature\"].map(feature_name_map)\n",
    "\n",
    "# 繪製特徵重要性的水平條形圖，重要性較高的特徵會顯示在最上面\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(\n",
    "    feature_importance[\"Feature\"][:10],  # 更新為中文特徵名稱\n",
    "    feature_importance[\"Importance\"][:10],\n",
    ")\n",
    "plt.xlabel(\"重要性(Importance)\")\n",
    "plt.ylabel(\"特徵(Feature)\")\n",
    "plt.title(\"前 10 特徵重要性 - XGBoost\")\n",
    "plt.gca().invert_yaxis()  # 確保重要性較高的特徵顯示在最上面\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高斯貝式分類器 GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 訓練貝葉斯分類器\n",
    "Bayesion_classifier_model = GaussianNB()\n",
    "Bayesion_classifier_model.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred_bc = Bayesion_classifier_model.predict(X_test)\n",
    "\n",
    "# 計算準確率和顯示分類報告\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "accuracy_bc = accuracy_score(y_test, y_pred_bc)\n",
    "classification_report_bc = classification_report(y_test, y_pred_bc)\n",
    "\n",
    "print(accuracy_bc)\n",
    "print(classification_report_bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 忽略警告用的\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# 定义参数范围\n",
    "param_grid = {\n",
    "    \"var_smoothing\": np.logspace(\n",
    "        0, -15, num=100\n",
    "    )  # 控制模型對於數據中的噪聲的容忍度，通過向變異數中加入一個小的值來穩定計算過程\n",
    "}\n",
    "\n",
    "# 建立 GridSearchCV 對象\n",
    "grid_search_bc = GridSearchCV(\n",
    "    GaussianNB(),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "\n",
    "# 執行網格搜索\n",
    "grid_search_bc.fit(X_train, y_train)\n",
    "\n",
    "# 輸出最佳參數和最佳交叉驗證分數\n",
    "print(\"Best parameters:\", grid_search_bc.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search_bc.best_score_)\n",
    "\n",
    "\n",
    "# 繪製結果\n",
    "mean_scores = grid_search_bc.cv_results_[\n",
    "    \"mean_test_score\"\n",
    "]  # 獲取每個參數的平均測試分數\n",
    "smoothing_values = param_grid[\"var_smoothing\"]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(smoothing_values, mean_scores)\n",
    "plt.scatter(\n",
    "    grid_search_bc.best_params_[\"var_smoothing\"],\n",
    "    grid_search_bc.best_score_,\n",
    "    color=\"red\",\n",
    "    marker=\"o\",\n",
    "    label=\"Best parameter\",\n",
    ")  # 標記最佳參數\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Var Smoothing (對數刻度)\")\n",
    "plt.ylabel(\"Accuracy (準確率)\")\n",
    "plt.title(\"GridSearchCV Var Smoothing vs. Accuracy (準確率)\")\n",
    "plt.legend()  # 顯示圖例\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用最佳參數來做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# 使用 GridSearchCV 找到的最佳參數\n",
    "best_var_smoothing = grid_search_bc.best_params_[\"var_smoothing\"]\n",
    "\n",
    "# 建立一個新的 GaussianNB 模型，使用最佳的 var_smoothing 參數\n",
    "Bayesion_classifier_model_best = GaussianNB(var_smoothing=best_var_smoothing)\n",
    "\n",
    "# 使用訓練數據訓練模型\n",
    "Bayesion_classifier_model_best.fit(X_train, y_train)\n",
    "\n",
    "y_pred_bc_best = Bayesion_classifier_model_best.predict(X_test)\n",
    "# 計算準確率\n",
    "accuracy_bc_best = accuracy_score(y_test, y_pred_bc_best)\n",
    "\n",
    "# 生成分類報告\n",
    "classification_report_bc_best = classification_report(y_test, y_pred_bc_best)\n",
    "\n",
    "# 打印結果\n",
    "print(\"最佳 GaussianNB 模型的準確度:\", accuracy_bc_best)\n",
    "print(classification_report_bc_best)\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_bc_best)\n",
    "\n",
    "\n",
    "\"\"\"交叉驗證(Cross-Validation)\"\"\"\n",
    "# 進行5-fold交叉驗證\n",
    "scores = cross_val_score(Bayesion_classifier_model_best, X_train, y_train, cv=5)\n",
    "\n",
    "\n",
    "# 輸出每一輪的準確率以及平均準確率\n",
    "print(\"每一輪的準確率:\", scores)\n",
    "print(\"平均準確率:\", scores.mean())\n",
    "\n",
    "\"\"\"輸出模型\"\"\"\n",
    "from joblib import dump, load\n",
    "\n",
    "# 保存模型\n",
    "dump(Bayesion_classifier_model_best, \"Bayesion_classifier_model_best.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_multi_class(y_test, y_pred_bc_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多項式貝氏分類器 MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 訓練貝葉斯分類器\n",
    "MultinomialNB_model = MultinomialNB()\n",
    "MultinomialNB_model.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred_mnb = MultinomialNB_model.predict(X_test)\n",
    "\n",
    "# 計算準確率和顯示分類報告\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "accuracy_mnb = accuracy_score(y_test, y_pred_mnb)\n",
    "classification_report_mnb = classification_report(y_test, y_pred_mnb)\n",
    "\n",
    "print(accuracy_mnb)\n",
    "print(classification_report_mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 參數調整(Parameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 忽略警告用的\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# 定义参数范围\n",
    "param_grid = {\n",
    "    # 'alpha': [0.01, 0.1, 1, 10, 100, 10000]\n",
    "    \"alpha\": np.logspace(-2, 14, num=10)\n",
    "    # 平滑參數，用於解決數據集中未見過的特徵所帶來的零概率問題\n",
    "}\n",
    "# 建立 GridSearchCV 對象\n",
    "grid_search_mnb = GridSearchCV(\n",
    "    MultinomialNB(),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "\n",
    "# 執行網格搜索\n",
    "grid_search_mnb.fit(X_train, y_train)\n",
    "\n",
    "# 輸出最佳參數和最佳交叉驗證分數\n",
    "print(\"Best parameters:\", grid_search_mnb.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search_mnb.best_score_)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 从grid_search_mnb获取alpha值和对应的准确率\n",
    "alphas = param_grid[\"alpha\"]\n",
    "mean_scores = grid_search_mnb.cv_results_[\"mean_test_score\"]\n",
    "\n",
    "# 绘制alpha对应准确率的图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas, mean_scores, marker=\"o\")\n",
    "# 确保这里使用了正确的最佳参数和分数\n",
    "plt.scatter(\n",
    "    grid_search_mnb.best_params_[\"alpha\"],\n",
    "    grid_search_mnb.best_score_,\n",
    "    color=\"red\",\n",
    "    marker=\"o\",\n",
    "    label=\"最佳参数\",\n",
    ")  # 标记最佳点\n",
    "plt.xlabel(\"Alpha (平滑参数)\")\n",
    "plt.ylabel(\"Accuracy (预测率)\")\n",
    "plt.title(\"MultinomialNB 不同 Alpha 的预测率\")\n",
    "plt.xscale(\"log\")  # 使用对数刻度\n",
    "plt.legend()  # 显示图例\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用最佳參數做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 忽略警告\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 使用找到的最佳參數建立一個新的 MultinomialNB 模型\n",
    "MultinomialNB_model_best = MultinomialNB(alpha=grid_search_mnb.best_params_[\"alpha\"])\n",
    "\n",
    "# 使用訓練數據訓練模型\n",
    "MultinomialNB_model_best.fit(X_train, y_train)\n",
    "\n",
    "# 使用測試數據進行預測\n",
    "y_pred_mnb_best = MultinomialNB_model_best.predict(X_test)\n",
    "\n",
    "# 計算準確率\n",
    "accuracy_mnb_best = accuracy_score(y_test, y_pred_mnb_best)\n",
    "\n",
    "# 生成分類報告\n",
    "classification_report_mnb_best = classification_report(y_test, y_pred_mnb_best)\n",
    "\n",
    "# 打印結果\n",
    "print(\"最佳 MultinomialNB 模型的準確度:\", accuracy_mnb_best)\n",
    "print(classification_report_mnb_best)\n",
    "\n",
    "# 繪製混淆矩陣\n",
    "plot_confusion_matrix(y_test, y_pred_mnb_best)\n",
    "# 進行5-fold交叉驗證\n",
    "scores = cross_val_score(MultinomialNB_model_best, X_train, y_train, cv=5)\n",
    "\n",
    "# 輸出每一輪的準確率以及平均準確率\n",
    "print(\"每一輪的準確率:\", scores)\n",
    "print(\"平均準確率:\", scores.mean())\n",
    "\n",
    "# 保存模型\n",
    "dump(MultinomialNB_model_best, \"MultinomialNB_model_best.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_multi_class(y_test, y_pred_mnb_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 訓練隨機森林模型\n",
    "randomforest_model = RandomForestClassifier(random_state=42)\n",
    "randomforest_model.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred_rf = randomforest_model.predict(X_test)\n",
    "\n",
    "# 計算準確率和顯示分類報告\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "classification_report_rf = classification_report(y_test, y_pred_rf)\n",
    "\n",
    "print(accuracy_rf)\n",
    "print(classification_report_rf)\n",
    "\n",
    "evaluate_model_multi_class(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 參數調整(Parameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# 定義參數範圍\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 400, 600],\n",
    "    \"max_depth\": [None, 5, 10, 20, 30],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [2, 4, 8, 16],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "'n_estimators'：您希望測試的決策樹數量。\n",
    "'max_depth'：每棵決策樹的最大深度。如果為 None，則表示沒有限制。\n",
    "'min_samples_split'：內部節點分裂所需的最小樣本數。\n",
    "'min_samples_leaf'：葉節點所需的最小樣本數。\n",
    "'max_features'：在尋找最佳分割時要考慮的特徵數量。\n",
    "\"\"\"\n",
    "\n",
    "# 定義隨機搜索的參數範圍\n",
    "param_dist = {\n",
    "    \"n_estimators\": sp_randint(100, 600),\n",
    "    \"max_depth\": [None, 5, 10, 20, 30],\n",
    "    \"min_samples_split\": sp_randint(2, 11),\n",
    "    \"min_samples_leaf\": sp_randint(1, 17),\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "}\n",
    "\n",
    "# 建立 RandomForestClassifier 模型\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 建立 GridSearchCV 對象\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "# 建立 RandomizedSearchCV 對象\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf, param_distributions=param_dist, n_iter=100, cv=5, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "# 執行網格搜索\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 執行隨機搜索\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 輸出結果\n",
    "print(\"GridSearchCV 最佳參數:\", grid_search.best_params_)\n",
    "print(\"GridSearchCV 最佳交叉驗證分數:\", grid_search.best_score_)\n",
    "print(\"RandomizedSearchCV 最佳參數:\", random_search.best_params_)\n",
    "print(\"RandomizedSearchCV 最佳交叉驗證分數:\", random_search.best_score_)\n",
    "\n",
    "# 比較並輸出最好的參數\n",
    "if grid_search.best_score_ > random_search.best_score_:\n",
    "    print(\"總體最佳參數來自 GridSearchCV:\", grid_search.best_params_)\n",
    "    best_params = grid_search.best_params_\n",
    "else:\n",
    "    print(\"總體最佳參數來自 RandomizedSearchCV:\", random_search.best_params_)\n",
    "    best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較並輸出最好的參數\n",
    "if grid_search.best_score_ > random_search.best_score_:\n",
    "    print(\"總體最佳參數來自 GridSearchCV:\", grid_search.best_params_)\n",
    "    best_params = grid_search.best_params_\n",
    "else:\n",
    "    print(\"總體最佳參數來自 RandomizedSearchCV:\", random_search.best_params_)\n",
    "    best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"用最佳參數在做一遍\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# 建立模型並設置參數\n",
    "# 重跑要一小時(用補習班的)\n",
    "\n",
    "best_randomforest_model = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=10,\n",
    "    max_features=\"sqrt\",\n",
    "    min_samples_leaf=8,\n",
    "    min_samples_split=2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "# # 建立模型並設置參數\n",
    "# best_randomforest_model = RandomForestClassifier(n_estimators=n_estimators_value,\n",
    "#     max_depth=best_params['max_depth'],\n",
    "#     max_features=best_params['max_features'],\n",
    "#     min_samples_leaf=best_params['min_samples_leaf'],\n",
    "#     min_samples_split=best_params['min_samples_split'],\n",
    "#     random_state=42)\n",
    "\n",
    "\n",
    "# 訓練模型\n",
    "\n",
    "best_randomforest_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 使用最佳模型進行預測\n",
    "\n",
    "y_pred_best = best_randomforest_model.predict(X_test)\n",
    "\n",
    "\n",
    "# 再次生成混淆矩陣並視覺化\n",
    "\n",
    "cm_best = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "cm_best_ratio = cm_best.astype(\"float\") / cm_best.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "# 使用Seaborn畫出混淆矩陣的熱力圖\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "sns.heatmap(cm_best_ratio, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
    "\n",
    "plt.title(\"Optimized RandomForest Confusion Matrix\")\n",
    "\n",
    "plt.ylabel(\"Actual label\")\n",
    "\n",
    "plt.xlabel(\"Predicted label\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\"\"\"交叉驗證\"\"\"\n",
    "\n",
    "# 忽略警告用的\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# 定義模型\n",
    "\n",
    "rf_model_cv = best_randomforest_model\n",
    "\n",
    "\n",
    "# 進行5-fold交叉驗證\n",
    "\n",
    "scores = cross_val_score(rf_model_cv, X_train, y_train, cv=5)\n",
    "\n",
    "\n",
    "# 輸出每一輪的準確率以及平均準確率\n",
    "\n",
    "print(\"每一輪的準確率:\", scores)\n",
    "\n",
    "print(\"平均準確率:\", scores.mean())\n",
    "\n",
    "\"\"\"輸出模型\"\"\"\n",
    "from joblib import dump\n",
    "\n",
    "# 保存模型\n",
    "dump(best_randomforest_model, \"best_randomforest_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_best_RF = best_randomforest_model.predict(X_test)\n",
    "evaluate_model_multi_class(y_test, y_pred_best_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chat.openai.com/c/5148fbf7-f239-4393-876b-aa680d7293c5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
